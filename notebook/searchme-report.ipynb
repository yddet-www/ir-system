{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab10a1d",
   "metadata": {},
   "source": [
    "# Project Report: Searchme\n",
    "\n",
    "Wikrama W. Wardhana, Illinois Institute of Technology, wwardhana@hawk.illinoistech.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb641",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This report presents Searchme, a web-based information retrieval system implementing the vector space model with TF-IDF term weighting and cosine similarity ranking. The system comprises three integrated components: a Scrapy-based web crawler configured for breadth-first traversal with politeness controls, a multithreaded indexer that constructs inverted indices with positional information and bigram character indices for spelling correction, and a query processor that handles natural language queries through tokenization, correction, and ranked retrieval. The indexer employs magnitude caching to optimize repeated cosine similarity calculations, while the bigram index enables approximate string matching for handling misspelled queries. Searchme was evaluated on the Cranfield benchmark dataset of 1,400 documents and 225 queries, achieving a Mean Average Precision of 0.27, Precision@10 of 0.22, and Recall@100 of 0.70. These results passes simple baseline systems but fall short of modern ranking functions like BM25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d5e1d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The objective of this project is to create an information retrieval system akin to a search engine. To mimic this behaviour, several key characteristics are required to consider the implementation as a derivative from such systems. The three core components this system employs: a Scrapy-based web crawler for document collection, a multithreaded indexer that builds tf-idf weighted inverted indices, and a query processor that ranks documents using cosine similarity. The system was evaluated against the Cranfield dataset to measure retrieval effectiveness using standard IR metrics.\n",
    "\n",
    "### Document Corpus Collection\n",
    "\n",
    "For an information retrieval system to function, it must first have access to the corpus containing said information. In the context of search engines, the system must therefore have a collection of website information and its content stored and preprocessed for extraction; the relevant information is then transformed into its appropriate data structures for the system. This was achieved through the use of [Scrapy's](https://www.scrapy.org/) web crawler to collect and download web pages for indexing. The crawler accepts a seed URL along with preconfigured parameters for maximum crawl depth and page limits, ensuring all documents are stored locally in HTML format. This approach allows the system to build a custom corpus tailored to specific topics. The crawler generates both the raw HTML files and a mapping file (url_map.jsonl) that associates document identifiers with their original URLs. Implementation details, including crawling strategies and storage organization, are discussed in the [Architecture](#architecture) section.\n",
    "\n",
    "### Corpus Processing and Inverted Index Creation\n",
    "\n",
    "Once an information retrieval system has access to its corpus of documents, its next step is to process its documents along with the content into the relevant data structures that builds the core of the entire system. In this system, the [\"Indexer\"](../src/indexer/indexme.py) serves that exact purpose: it transforms raw HTML documents into searchable data structures. It parses HTML content using BeautifulSoup, extracts only the texts, and tokenizes it while filtering stopwords and punctuation. The core data structure is an inverted index that maps terms to document identifiers and positional information, represented using tf-idf weights. To improve performance, the indexer employs multithreaded processing that distributes tokenization across CPU cores. Key design decisions regarding tokenization strategies, threading implementation, and index persistence are detailed in the [Architecture](#architecture) section.\n",
    "\n",
    "### Query Processing and Document Retrieval\n",
    "\n",
    "The last key component is the user-facing interface that handles input queries and perform the necessary preprocessing before executing the information retrieval pipeline. It accepts natural language queries, applies the same preprocessing pipeline used during indexing (tokenization, stopword removal, case normalization), and generates a query vector using tf-idf weighting. The system then computes cosine similarity scores between the query vector and all candidate documents, ranking them by relevance. To manage computational load and improve user experience, Searchme implements top-K retrieval, returning only the most relevant documents. The system also accommodates user input variations through its bigram index, which supports wildcard matching for misspelled or partially-known terms. The [Design](#design) section outlines the complete information retrieval pipeline from query input to ranked result presentation, while implementation specifics are provided in the [Architecture](#architecture) section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe257f",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "This section describes the overarching design of Searchme's information retrieval system, describing its system capabilities, component interactions, and key design decisions that shaped the implementation.\n",
    "\n",
    "### System Capabilities\n",
    "\n",
    "Searchme provides several core capabilities that enable effective information retrieval from a collection of web documents:\n",
    "\n",
    "#### Index Construction\n",
    "\n",
    "The system processes HTML documents to build an inverted index with tf-idf weighted term representations. Each term in the index maintains positional information, recording the exact locations where terms appear within documents to enable future extensions, such as phrase queries or proximity-based ranking.\n",
    "\n",
    "#### Query Processing\n",
    "\n",
    "Users can submit queries that are tokenized and processed using the same preprocessing pipeline applied during indexing. The system converts queries into tf-idf weighted vectors and computes cosine similarity scores against all indexed documents to determine relevance.\n",
    "\n",
    "#### Ranked Retrieval\n",
    "\n",
    "Rather than returning all matching documents, Searchme implements top-K retrieval to present only the most relevant results calculated by their cosine similarity scores.\n",
    "\n",
    "#### Wildcard Query Support\n",
    "\n",
    "Through a bigram character index, the system supports wildcard queries that can match terms with partial or uncertain spellings. This accommodates user input variations and helps retrieve relevant documents even when exact term matches are not found.\n",
    "\n",
    "#### Efficient Processing\n",
    "\n",
    "The system uses multithreaded document processing to leverage multiple CPU cores during index construction, and caches document magnitude calculations to avoid redundant computations and speed up query evaluation.\n",
    "\n",
    "### Component Interactions and Data Flow\n",
    "\n",
    "The web crawler serves as the entry point to the system. Given the preconfigured parameters of a seed URL, maximum depth, and page limit, it traverses the web by following hyperlinks and downloading HTML content. The crawler outputs two artifacts: a directory of HTML files named by unique document identifiers, and a JSON Lines file that preserves the its document IDs and original URLs mapping. This separation allows the indexer to process documents efficiently while maintaining traceability to source locations for later components to use.\n",
    "\n",
    "The indexer reads HTML files from the corpus directory and transforms them into searchable data structures. Each document passes through a processing pipeline that extracts text content using HTML parsing, converts text to lowercase for case-insensitive matching, removes punctuation through regular expression substitution, and filters common stopwords that provide little informational value. The resulting tokens are organized into an inverted index where each term maps to a posting list containing document IDs and their respective positions for when it appears. Simultaneously, a bigram index is constructed by generating character bigrams for each term, enabling wildcard matching capabilities. The complete index structure, along with corpus metadata, is serialized to JSON format for persistence and future loading.\n",
    "\n",
    "When a user submits a query, the processor applies the same tokenization and preprocessing steps used during indexing to ensure consistent term representation. The processed query tokens are converted into a tf-idf weighted vector by computing term frequencies within the query and multiplying by inverse document frequency values from the index. The system then iterates through the inverted index, computing dot products between the query vector and document vectors for all candidate documents containing at least one query term. These raw dot product scores are normalized by the magnitudes of both the query vector and each document vector to produce cosine similarity scores bounded between zero and one. Finally, documents are sorted by score in descending order, and the top-K results are returned to the user.\n",
    "\n",
    "### Integration\n",
    "\n",
    "The three components are loosely coupled through file system conventions and data formats. The crawler and indexer communicate through a shared directory structure where HTML files and the URL mapping file reside. The indexer and query processor share access to the serialized index file, which contains all necessary data structures for retrieval. This file-based integration approach provides several advantages: components can be developed and tested independently, the system naturally supports batch processing where corpus collection and indexing occur offline while query processing happens online, and the serialized index serves as a checkpoint that eliminates the need to rebuild indices for repeated query sessions. The indexer implements lazy loading, creating the index only when no existing index file is found. This design allows the system to skip expensive index construction when working with a previously processed corpus, reducing startup time for query processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c034a9",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "This following section will detail code implementations across the three core components, pulling snippets from the source code, noting key design decisions and its implications, and how it integrates with the entire system.\n",
    "\n",
    "### Web crawler\n",
    "\n",
    "```python\n",
    "custom_settings: dict[str, Any] = {\n",
    "    \"DEPTH_LIMIT\": 3,\n",
    "    \"CLOSESPIDER_PAGECOUNT\": 5000,\n",
    "    \"AUTOTHROTTLE_ENABLED\": True,\n",
    "    \"AUTOTHROTTLE_START_DELAY\": 1,\n",
    "    \"AUTOTHROTTLE_MAX_DELAY\": 5,\n",
    "    \"AUTOTHROTTLE_TARGET_CONCURRENCY\": 1.5,\n",
    "    \"CONCURRENT_REQUESTS\": 16,\n",
    "    \"CONCURRENT_REQUESTS_PER_DOMAIN\": 2,\n",
    "    \"ROBOTSTXT_OBEY\": True,\n",
    "}\n",
    "```\n",
    "\n",
    "The web crawler is preconfigured with values that best support the functions of a search engine. Namely, Searchme settled on using a small `DEPTH_LIMIT` of 3 combined with a significantly larger `CLOSESPIDER_PAGECOUNT` of 5000. This configuration encourages breadth-first traversal, where the crawler explores many pages at shallow depths rather than following deeply nested link chains. This approach captures diverse content across the crawled site(s) without getting trapped in deep hierarchical structures or infinite link loops. And to assist the crawling process, several autothrottling and concurrent settings are configured as to better optimize the process and maintain politeness of target websites so as to not overload them with requests.\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def only_http_https(url: str):\n",
    "    scheme = urlparse(url).scheme.lower()\n",
    "    if scheme in {\"\", \"http\", \"https\"}:\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "link_extractor = LinkExtractor(\n",
    "    process_value=only_http_https,\n",
    "    allow_domains=[\"en.wikipedia.org\"],\n",
    "    deny=[\n",
    "        r\"/user/\",\n",
    "        r\"/profile/\",\n",
    "        r\"/account/\",\n",
    "        r\"/login\",\n",
    "        r\"/register\",\n",
    "        r\"/signup\",\n",
    "        r\"\\?action=\",\n",
    "        r\"/edit\",\n",
    "        r\"/delete\",\n",
    "        r\"/admin\",\n",
    "        r\"/tag/\",\n",
    "        r\"/category/\",\n",
    "        r\"/archive/\",\n",
    "        r\"/search\",\n",
    "        r\"/share\",\n",
    "        r\"/print\",\n",
    "        r\"/comment\",\n",
    "        r\"#comment\",\n",
    "    ],\n",
    "    deny_extensions=[\"pdf\", \"zip\", \"gz\", \"tar\", \"7z\", \"rar\"],\n",
    "    tags=[\"a\"],\n",
    "    attrs=[\"href\"],\n",
    "    canonicalize=True,\n",
    "    unique=True,\n",
    ")\n",
    "```\n",
    "\n",
    "To better control the behaviour of Searchme's crawler, the spider utilizes Scrapy's `LinkExtractor` object to parse through hyperlinks and filter out candidate hops between pages. For the purposes of this system, it is limited within the domain of `en.wikipedia.org` as a promising seeding URL resulting in a sufficient corpus to test robustness of the entire system. Morever, the link extractor employs further URL parsing to ensure only HTTP/HTTPS web pages are crawled, ignoring common page links that serve little to no informational gain, files nested for download within the pages themselves that is not supported by this system, and strictly download a specific URL once to ensure no duplicates are downloaded.\n",
    "\n",
    "```python\n",
    "@classmethod\n",
    "def from_crawler(cls, crawler, *args, **kwargs):\n",
    "    output_path_arg = kwargs.get(\"output_path\", cls.default_output_path)\n",
    "    output_path = Path(output_path_arg)\n",
    "\n",
    "    feeds = {\n",
    "        str(output_path / \"url_map.jsonl\"): {\"format\": \"jsonlines\"},\n",
    "    }\n",
    "    crawler.settings.set(\"FEEDS\", feeds, priority=\"spider\")\n",
    "\n",
    "    spider = super().from_crawler(crawler, *args, **kwargs)\n",
    "    return spider\n",
    "\n",
    "```\n",
    "\n",
    "The spider is then configured to maintain a trace of all downloaded HTML pages by creating a JSON line file that maps their respective URL and the parsed document ID, performed by the parser of the same spider.\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "        self.log(f\"Scraped URL @ {response.url}\")\n",
    "        docID = str(uuid.uuid5(uuid.NAMESPACE_URL, response.url))\n",
    "\n",
    "        # save for mapping (goes into url_map.jsonl)\n",
    "        yield {\"docID\": docID, \"url\": response.url}\n",
    "\n",
    "        html_output_path = self.output_path / \"html\"\n",
    "        filename = f\"{docID}.html\"\n",
    "\n",
    "        html_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        (html_output_path / filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "\n",
    "        links = self.link_extractor.extract_links(response)\n",
    "        yield from response.follow_all(links, callback=self.parse)\n",
    "```\n",
    "\n",
    "Finally, the parser ties the previous together. All scraped URLs are transformed into a unique UUID as their document ID representaion, and its mapping is then written into the aforementioned JSON line file. Then, all scraped hyperlink elements from the downloaded HTML are passed through the link extractor object previously defined to be filtered so as to minimize bad crawls, and the spider will recursively traverse each links until it reaches the page count limits.\n",
    "\n",
    "### Indexer\n",
    "\n",
    "```python\n",
    "def init_invidx_tfdf(doc_idx: dict[str, list[str]]):\n",
    "    inverted_idx: dict[str, dict[str, list[int]]] = {}\n",
    "\n",
    "    for doc, term_lst in doc_idx.items():\n",
    "\n",
    "        # maintain list of positions of term per document\n",
    "        # note it is always sorted as it assumes list of text is in original ordering\n",
    "        for pos, term in enumerate(term_lst):\n",
    "            if term in inverted_idx:\n",
    "                posting_list = inverted_idx[term].setdefault(doc, [])\n",
    "                posting_list.append(pos)\n",
    "            else:\n",
    "                inverted_idx[term] = {doc: [pos]}\n",
    "\n",
    "    return inverted_idx\n",
    "```\n",
    "\n",
    "The core of Searchme's indexer lies in the inverted index creation function. Simply put, given a dictionary of documet IDs and their respective list of tokens, the function iterates through each document and created the inverted index consisting of unique terms while noting each documents containing said terms. Moreover, each position of the terms are recorded for each document to enable future expansion for positional information processing when querying.\n",
    "\n",
    "```python\n",
    "# Attr:\n",
    "# https://stackoverflow.com/questions/312443/how-do-i-split-a-list-into-equally-sized-chunks\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "def html_tokenizer(html_path: str):\n",
    "    p = Path(html_path)\n",
    "\n",
    "    with open(p, \"rb\") as html:\n",
    "        soup_obj = BeautifulSoup(html, \"html.parser\")\n",
    "        content = soup_obj.get_text(\" \", strip=True).lower()\n",
    "\n",
    "    regex_p = r\"[^\\w\\s]\"\n",
    "    tokens = re.sub(regex_p, \"\", content).split()\n",
    "    filtered_tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def ptokenize(html_list: list[Path], doc_idx: dict[str, list[str]]):\n",
    "    for path in html_list:\n",
    "        token_list = html_tokenizer(str(path))\n",
    "        doc_idx[path.stem] = token_list\n",
    "```\n",
    "\n",
    "To better optimize the system during this index creation, the process of tokenizing each document into their document-token list dictionary is threaded and made adaptive based on the number of the system's CPU cores. This is done to speed up the processing time of HTML file reads and processing potentially thousands of pages.\n",
    "\n",
    "```python\n",
    "class Index:\n",
    "    def __init__(\n",
    "        self, idx_file: Path = Path(INV_IDX_FILE), corpus_path: Path = CRWL_OUTPUT\n",
    "    ):\n",
    "        self.index_path: Path = INDX_OUTPUT / idx_file\n",
    "        self._magnitude_cache = {}  # inshallah this speeds up cosine search\n",
    "\n",
    "        if not self.index_path.exists():\n",
    "            print(f\"No inverted index found @ {self.index_path}, creating one...\")\n",
    "            self.create_index(corpus_path)\n",
    "        else:\n",
    "            self.load_index()\n",
    "\n",
    "    def create_index(self, corpus_path: Path):\n",
    "        if self.index_path.exists():\n",
    "            raise FileExistsError(\n",
    "                f\"File already exists at {str(self.index_path)}\\n     Use load_index() to load file into object instead.\"\n",
    "            )\n",
    "\n",
    "        self.corpus_path: Path = corpus_path\n",
    "        self.corpus_mapping: Path = self.corpus_path / \"url_map.jsonl\"\n",
    "\n",
    "        # run the crawler first, moron\n",
    "        if not self.corpus_mapping.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f'Expected a \"url_map.jsonl\" file at @ {self.corpus_mapping}, found none'\n",
    "            )\n",
    "\n",
    "        # sort so its not so random for my sake\n",
    "        docs = sorted((self.corpus_path / \"html\").iterdir())\n",
    "        self.corpus_size: int = len(docs)\n",
    "\n",
    "        chunk_size = len(docs) // CPU_COUNT\n",
    "        document_chunks = chunks(docs, chunk_size)\n",
    "\n",
    "        threads = []\n",
    "        doc_idx: dict[str, list[str]] = {}\n",
    "        for chunk in document_chunks:\n",
    "            t = threading.Thread(target=ptokenize, args=(chunk, doc_idx))\n",
    "            threads.append(t)\n",
    "\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        self.inverted_index: dict[str, dict[str, list[int]]] = init_invidx_tfdf(doc_idx)\n",
    "        self.bigram_index: dict[str, list[str]] = init_bigram_idx(\n",
    "            list(self.inverted_index.keys())\n",
    "        )\n",
    "        Path(INDX_OUTPUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(self.index_path, \"w\") as idx_file:\n",
    "            idx_obj = {\n",
    "                \"index_path\": str(self.index_path),\n",
    "                \"corpus_path\": str(self.corpus_path),\n",
    "                \"corpus_mapping\": str(self.corpus_mapping),\n",
    "                \"corpus_size\": self.corpus_size,\n",
    "                \"inverted_index\": self.inverted_index,\n",
    "                \"bigram_index\": self.bigram_index,\n",
    "            }\n",
    "            json.dump(idx_obj, idx_file, indent=2)\n",
    "\n",
    "    def load_index(self):\n",
    "        if not self.index_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"File not found @ {str(self.index_path)}\\n Use create_index() to generate a new inverted index file.\"\n",
    "            )\n",
    "\n",
    "        with open(self.index_path, \"r\") as idx_file:\n",
    "            idx_obj = json.load(idx_file)\n",
    "\n",
    "        self.index_path = Path(idx_obj[\"index_path\"])\n",
    "        self.corpus_path = Path(idx_obj[\"corpus_path\"])\n",
    "        self.corpus_mapping = Path(idx_obj[\"corpus_mapping\"])\n",
    "        self.corpus_size = idx_obj[\"corpus_size\"]\n",
    "        self.inverted_index = idx_obj[\"inverted_index\"]\n",
    "        self.bigram_index = idx_obj[\"bigram_index\"]\n",
    "```\n",
    "\n",
    "To integrate all the loosely defined functions together, the Indexer class behaves as the central pipeline for inverted index creation. If a given path to a non-existent inverted index serialized into a JSON file, the index creation pipeline is invoked. The system will look for the directory of HTML files from which the crawler downloaded its results and processes each file to construct the intermediate document-token data structure. Then, it transforms that into the complete inverted index.\n",
    "\n",
    "```python\n",
    "def get_idf(self, term: str):\n",
    "    if term not in self.inverted_index:\n",
    "        return 0\n",
    "\n",
    "    n = self.corpus_size\n",
    "    df = len(self.inverted_index[term])\n",
    "\n",
    "    idf = log10(n / df)\n",
    "    return idf\n",
    "\n",
    "def get_tf(self, term: str, doc: str):\n",
    "    if term not in self.inverted_index:\n",
    "        return 0\n",
    "\n",
    "    return len(self.inverted_index[term][doc])\n",
    "```\n",
    "\n",
    "From the same class, the system can retrieve the required TF-IDF values on query-time, which will be necessary on the next cosine similarity score calculation.\n",
    "\n",
    "```python\n",
    "def cosine_search(self, query_tokens: list[str], k: int = 10):\n",
    "    query_vector = {}\n",
    "    for term in query_tokens:\n",
    "        if term in self.inverted_index:\n",
    "            tf = query_tokens.count(term)\n",
    "            idf = self.get_idf(term)\n",
    "            query_vector[term] = tf * idf\n",
    "\n",
    "    doc_scores: dict[str, float] = {}\n",
    "    for term in query_vector:\n",
    "        for doc_id in self.inverted_index[term]:\n",
    "            # doc vector component\n",
    "            doc_tf = self.get_tf(term, doc_id)\n",
    "            doc_tfidf = doc_tf * self.get_idf(term)\n",
    "\n",
    "            # dot product component\n",
    "            doc_scores[doc_id] = (\n",
    "                doc_scores.get(doc_id, 0) + query_vector[term] * doc_tfidf\n",
    "            )\n",
    "\n",
    "    query_magnitude = sum(v**2 for v in query_vector.values()) ** 0.5\n",
    "\n",
    "    # normalizing\n",
    "    for doc_id in doc_scores:\n",
    "        doc_magnitude = self._get_doc_magnitude(doc_id)\n",
    "        doc_scores[doc_id] /= query_magnitude * doc_magnitude\n",
    "\n",
    "    # best to worst\n",
    "    ranked = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:k]\n",
    "```\n",
    "\n",
    "The final key implementation lies in the cosine search calculation. It expects a list of a tokenized query and represent it into its weighted TF-IDF vector form. The same process follows for all candidate documents (at least one term from query present in the document), and calculate each of its dot product with respect to the query vector. The resulting value is then normalized by the magnitude of the query and document vectors. A point of concern that became painfully obvious when running this calculation was the execution time for summing each document magnitude necessary for the normalization. To optimize this calculation, the system employs a simple caching as follows.\n",
    "\n",
    "```python\n",
    "def _get_doc_magnitude(self, doc_id: str):\n",
    "    if doc_id in self._magnitude_cache:\n",
    "        return self._magnitude_cache[doc_id]\n",
    "\n",
    "    magnitude_squared = 0\n",
    "\n",
    "    # for every term in this document\n",
    "    for term in self.inverted_index:\n",
    "        if doc_id in self.inverted_index[term]:\n",
    "            tf = self.get_tf(term, doc_id)\n",
    "            idf = self.get_idf(term)\n",
    "            tfidf = tf * idf\n",
    "            magnitude_squared += tfidf**2\n",
    "\n",
    "    magnitude = magnitude_squared**0.5  # sqrt\n",
    "\n",
    "    self._magnitude_cache[doc_id] = magnitude  # put in the cache\n",
    "    return magnitude\n",
    "```\n",
    "\n",
    "Within the Index class exists a variable that stores all calculated magnitude values of a document, and the values are updated whenever the function `_get_doc_magnitude` is invoked. This will first check the cache for any existing value of that specific document before proceeding with its expensive calculation. This significantly reduces time taken for sequential queries an theoretically eliminates the document magnitude calculation bottleneck as the system continues to function.\n",
    "\n",
    "```python\n",
    "def bigram(term: str):\n",
    "    gram_set = set()\n",
    "    togram = term + \"$\"\n",
    "    curr = \"$\"\n",
    "\n",
    "    for char in togram:\n",
    "        curr += char\n",
    "\n",
    "        if not (\n",
    "            \"$*\" == curr or \"*$\" == curr\n",
    "        ):  # Remove k-grams for when the `$` is right beside `*`, i.e., wilcard begins at the beginning or end\n",
    "            gram_set.add(curr)\n",
    "\n",
    "        curr = curr[1:]\n",
    "\n",
    "    # Filter out wildcard bigrams\n",
    "    gram_set = {item for item in gram_set if \"*\" not in item}\n",
    "\n",
    "    return gram_set\n",
    "\n",
    "\n",
    "def init_bigram_idx(terms: list[str]):\n",
    "    kgram_idx: dict[str, list[str]] = {}\n",
    "\n",
    "    for t in terms:\n",
    "        bigrams = bigram(t)\n",
    "\n",
    "        for b in bigrams:\n",
    "            if b in kgram_idx:\n",
    "                kgram_idx[b].append(t)\n",
    "            else:\n",
    "                kgram_idx.setdefault(b, [t])\n",
    "\n",
    "    return kgram_idx\n",
    "```\n",
    "\n",
    "The bigram index serves as a supporting data structure for query correction and wildcard matching capabilities in the search interface. Each term in the inverted index is decomposed into character bigrams. These bigrams are then indexed to map back to their source terms, creating a reverse lookup structure. This enables the system to handle misspelled queries or partial term matches by comparing the bigrams of a query term against the bigram index to find the closest matching terms in the vocabulary. When a user submits a query with typos or uses wildcard patterns, the search component can leverage this bigram index to suggest corrections or expand wildcards into concrete terms that exist in the corpus.\n",
    "\n",
    "### Search\n",
    "\n",
    "The search component serves as the user-facing interface that orchestrates the entire information retrieval pipeline. When a user submits a query, it undergoes several preprocessing steps before retrieval and ranking.\n",
    "\n",
    "```python\n",
    "def tokenizer(query: str):\n",
    "    query_lowercase = query.lower()\n",
    "    regex_p = r\"[^\\w\\s]\"\n",
    "    tokens = re.sub(regex_p, \"\", query_lowercase).split()\n",
    "    filtered_tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "    return filtered_tokens\n",
    "```\n",
    "\n",
    "The tokenizer function applies the same text processing pipeline used during indexing: converting thequery to lowercase for case-insensitive matching, removing punctuation throughregular expression substitution, splitting on whitespace, and filtering common stopwords. This consistent preprocessing ensures that query terms are represented in the same form as indexed terms, enabling accurate matching in the inverted index.\n",
    "\n",
    "```python\n",
    "def query_correction(query: list[str], bigram_idx: dict[str, list[str]]):\n",
    "    corrected_query: list[str] = []\n",
    "    correction_flag = False  # if correction occured, flip the flag\n",
    "\n",
    "    for query_term in query:\n",
    "        bigrams = bigram(query_term)\n",
    "\n",
    "        # all candidate terms that match the bigrams\n",
    "        candidate_terms: set[str] = set()\n",
    "        for bg in bigrams:\n",
    "            if bg in bigram_idx:\n",
    "                candidate_terms.update(bigram_idx[bg])\n",
    "\n",
    "        # no candidates found, keep original term\n",
    "        if not candidate_terms:\n",
    "            corrected_query.append(query_term)\n",
    "            continue\n",
    "\n",
    "        # best match\n",
    "        min_distance = float(\"inf\")\n",
    "        best_match = query_term\n",
    "\n",
    "        for candidate in candidate_terms:\n",
    "            lev_dist = edit_distance(candidate, query_term)\n",
    "\n",
    "            # exact match\n",
    "            if lev_dist == 0:\n",
    "                best_match = candidate\n",
    "                break\n",
    "\n",
    "            # update best match\n",
    "            if lev_dist < min_distance:\n",
    "                min_distance = lev_dist\n",
    "                best_match = candidate\n",
    "\n",
    "        if best_match != query_term:\n",
    "            correction_flag = True\n",
    "\n",
    "        corrected_query.append(best_match)\n",
    "\n",
    "    return corrected_query, correction_flag\n",
    "```\n",
    "\n",
    "The query correction function leverages the bigram index to handle misspelled or mistyped query terms. For each term in the tokenized query, the function generates its character bigrams and looks up candidate terms in the bigram index that share those bigrams. If no candidates are found, the original term is retained unchanged. When candidates exist, the system computes the Levenshtein edit distance between the query term and each candidate to identify the closest match. An edit distance of zero indicates an exact match, immediately selecting that term. Otherwise, the candidate with the minimum edit distance becomes the corrected term. This approach balances accuracy and efficiency by first narrowing the search space through bigram matching before applying the more expensive edit distance calculation. The function returns both the corrected query and a boolean flag indicating whether any corrections occurred, allowing the system to inform users when their query has been modified.\n",
    "\n",
    "```python\n",
    "def get_url_mapping(url_map_fp: Path) -> dict[str, str]:\n",
    "    if not url_map_fp.exists():\n",
    "        raise FileNotFoundError(f\"Expected file @ {url_map_fp}, but found nothing\")\n",
    "\n",
    "    with open(url_map_fp, \"r\") as f:\n",
    "        return {json.loads(line)[\"docID\"]: json.loads(line)[\"url\"] for line in f}\n",
    "```\n",
    "\n",
    "The URL mapping retrieval function provides the interface between internal document identifiers and their original web addresses. It reads the JSON Lines file produced by the web crawler, parsing each line to construct a dictionary that maps document UUIDs to their source URLs. This mapping is essential for presenting search results to users, as document IDs are meaningless without their corresponding web addresses. The function includes error handling to ensure the mapping file exists before attempting to read it, preventing runtime failures when the corpus has not been properly collected.\n",
    "\n",
    "```python\n",
    "def query_pipeline(query: str, index: Index, url_map: dict[str, str]):\n",
    "    Path(SRCH_LOGS).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    q_tokenized = tokenizer(query)\n",
    "\n",
    "    if not q_tokenized:\n",
    "        raise ValueError(\"Processed query is empty!\")\n",
    "\n",
    "    q_corrected, flag = query_correction(q_tokenized, index.bigram_index)\n",
    "    q_corrected_str = \" \".join(q_corrected)\n",
    "\n",
    "    documents = index.cosine_search(q_corrected)\n",
    "\n",
    "    # handle non-existent and empty conditions (if it ever happens)\n",
    "    file_exist = SRCH_LOGS_FP.exists() and SRCH_LOGS_FP.stat().st_size > 0\n",
    "\n",
    "    with open(SRCH_LOGS_FP, \"a\", newline=\"\") as query_log:\n",
    "        writer = csv.DictWriter(\n",
    "            query_log,\n",
    "            fieldnames=[\n",
    "                \"query\",\n",
    "                \"corrected_q\",\n",
    "                \"is_corrected\",\n",
    "                \"docid\",\n",
    "                \"url\",\n",
    "                \"score\",\n",
    "            ],\n",
    "            delimiter=\";\",\n",
    "        )\n",
    "\n",
    "        if not file_exist:\n",
    "            writer.writeheader()\n",
    "\n",
    "        for doc_id, score in documents:\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"corrected_q\": q_corrected_str,\n",
    "                    \"is_corrected\": flag,\n",
    "                    \"docid\": doc_id,\n",
    "                    \"url\": url_map[doc_id],\n",
    "                    \"score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return documents, q_corrected, flag\n",
    "```\n",
    "\n",
    "The query pipeline function integrates all previous components into a complete workflow. It accepts a raw user query, the index object, and the URL mapping, then orchestrates the following sequence: tokenization of the input query, correction of any misspelled terms using the bigram index, execution of the cosine similarity search to retrieve and rank relevant documents, and logging of the complete query session. The logging mechanism records the original query, corrected query, correction status, and all retrieved documents with their scores and URLs. This data is written to a CSV file with semicolon delimiters, maintaining a persistent record of system usage that can be analyzed for performance evaluation or debugging purposes. The function returns the ranked document list along with the corrected query and correction flag, providing the caller with all necessary information to present results to the user and indicate when automatic corrections have been applied. Error handling ensures that empty queries after tokenization raise an informative exception rather than silently failing during retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a41af6",
   "metadata": {},
   "source": [
    "## Operation\n",
    "\n",
    "Search me can be locally deployed by pulling the repository at `https://github.com/yddet-www/ir-system.git`.\n",
    "\n",
    "### Setup & Usage\n",
    "\n",
    "1. After cloning the repository, change your terminal's working directory to the root of the cloned project.\n",
    "\n",
    "   ```bash\n",
    "   cd /path/to/ir-system\n",
    "   ```\n",
    "\n",
    "2. With Python installed in the system, run:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "   This step will install the necessary packages used in Searchme's system. It is advised to first create a Python virtual environment beforehand.\n",
    "\n",
    "3. Since a fresh project will not contain any document corpus, we must first execute the system's web crawler. Users are free to modify [`urls.txt`](https://github.com/yddet-www/ir-system/blob/main/src/web-crawler/urls.txt) file to specify the spider's seed URL. To execute the spider:\n",
    "\n",
    "   ```bash\n",
    "   python -m scripts.web-crawler.init\n",
    "   ```\n",
    "\n",
    "   This executes a predefined script to run the spider with its default settings.\n",
    "\n",
    "4. Once the spider completes its execution, the system will now have its corpus of documents ready to begin its main pipeline. The search interface and inverted index creation/loading are all integrated under a single pipeline, and it initialize it:\n",
    "\n",
    "   ```bash\n",
    "   python -m scripts.start-searchme\n",
    "   ```\n",
    "\n",
    "   This script will launch the Flask-based application interface along with the core pipeline, loading the index into the system. The execution takes some time when ran for the first time as it needs to process the corpus and create an inverted index. To access the interface, open your browser of choice and visit http://127.0.0.1:5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8928d",
   "metadata": {},
   "source": [
    "## Test cases\n",
    "\n",
    "This section describes the testing framework, test coverage, and evaluation methodology employed to verify Searchme's correctness and retrieval effectiveness. The testing strategy encompasses three levels: unit testing of individual components, integration testing of the complete pipeline, and information retrieval evaluation using the Cranfield benchmark dataset.\n",
    "\n",
    "### Testing Framework\n",
    "\n",
    "Searchme's test suite is built using Python's unittest framework, organized into three primary test modules that correspond to the system's core components. All tests reside in a tests/ directory at the project root, with test fixtures stored in tests/fixtures/ to provide isolated, reproducible test environments. The test suite can be executed with standard unittest commands or with pytest for enhanced reporting capabilities.\n",
    "The testing infrastructure follows best practices for isolation and reproducibility. Each test class includes setup methods that initialize necessary resources such as test indices and corpus directories. Test fixtures include a small Wikipedia corpus of approximately 50 pages, a pre-built test index, and seed URLs for crawler validation. This controlled environment ensures tests run quickly and consistently across different systems without requiring large downloads or lengthy index construction.\n",
    "\n",
    "### Component Testing\n",
    "\n",
    "#### Web Crawler Tests (test_crawler.py)\n",
    "\n",
    "The web crawler test validates the complete crawling pipeline from seed URLs to downloaded HTML files. The test limits corpus collection to 50 pages to ensure reasonable test execution time. The crawler is seeded with Wikipedia URLs and configured to respect the same domain restrictions used in production.\n",
    "\n",
    "The test verifies several critical properties of the crawler output. First, it confirms that both the HTML directory and URL mapping file are created in the expected locations. Second, it validates the integrity of the URL mapping by ensuring every downloaded HTML file has a corresponding entry in url_map.jsonl. This bidirectional consistency check prevents orphaned files or missing mappings that would cause downstream failures in the indexer. The test leverages the get_url_mapping function from the search component, providing incidental integration testing of that utility function.\n",
    "\n",
    "#### Indexer Tests (test_indexer.py)\n",
    "\n",
    "The indexer test suite focuses on index construction correctness and persistence reliability. The primary test, test_index_identity, validates that creating a new index from a corpus produces results identical to a known-good reference index. This test first deletes any existing test index to force fresh construction, then creates a new index from the test corpus. It compares the newly created index against a pre-validated reference index stored in the fixtures directory.\n",
    "\n",
    "The comparison operates at multiple levels of granularity. At the structural level, the test verifies that both indices contain identical sets of terms in the inverted index and identical bigrams in the bigram index. This ensures the tokenization, stopword filtering, and index construction logic produces consistent results. The test uses set comparison for efficiency, as term order is irrelevant for index correctness.\n",
    "\n",
    "The test_idf_consistency test provides a deeper validation of index numerical properties. Rather than comparing entire data structures, it randomly samples 1000 terms from the vocabulary and verifies their IDF values match exactly between the newly created and reference indices. This statistical sampling approach balances thoroughness with execution speed, as computing IDF for every term would be expensive for large vocabularies.\n",
    "\n",
    "The IDF consistency test serves two purposes. It validates the correctness of IDF calculation logic and confirms that index serialization and deserialization preserve numerical precision. Any discrepancies in IDF values would indicate bugs in the TF-IDF calculation, corpus size tracking, or JSON serialization process. The choice to test IDF rather than raw term frequencies reflects the importance of IDF in the ranking function—incorrect IDF values directly impact retrieval quality, while TF errors might have more localized effects.\n",
    "\n",
    "#### Search Component Tests (test_search.py)\n",
    "\n",
    "The search component test suite validates query processing, correction, and the end-to-end retrieval pipeline. The setUp method initializes a test index and URL mapping that all search tests share, improving execution efficiency by avoiding repeated index loading.\n",
    "\n",
    "The test_tokenizer function validates query preprocessing through seven distinct test cases. These cases cover the full range of expected inputs: basic tokenization with stopword removal, case normalization, excessive whitespace handling, punctuation removal (including contractions), queries composed entirely of stopwords, and empty or whitespace-only queries. The test cases ensures the tokenizer handles edge cases gracefully, particularly the empty query scenario that could cause failures if not properly detected.\n",
    "\n",
    "The test_query_correction function validates the spelling correction using the bigram index. Three test scenarios exercise different correction behaviors: queries with misspellings that require correction (\"mattison\" → \"mathison\"), queries with correct spellings that should pass through unchanged (\"Alan Turing\"), and queries with multiple errors (\"hofstater\" → \"hofstadter\"). Each test verifies both the corrected query output and the boolean correction flag, ensuring the system can accurately inform users when their queries have been modified. This transparency is important for user trust and debugging.\n",
    "\n",
    "The test_query_pipeline function validates the complete end-to-end retrieval workflow. It submits a multi-term query and verifies the system returns exactly 10 results (the default top-K value) in descending score order. The score ordering validation is critical—incorrect ranking would render the entire system ineffective regardless of whether relevant documents are retrieved. The test includes two error condition checks: queries that tokenize to empty strings (all stopwords or whitespace) should raise ValueError exceptions with informative messages rather than silently failing or returning nonsensical results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8795e2",
   "metadata": {},
   "source": [
    "### Information Retrieval Metric Evaluation\n",
    "\n",
    "This subsection will walk through the evaluation procedures with live demo of core components in the system. For this purposes, the system employs Cranfield benchmark dataset.\n",
    "\n",
    "Provided from [ir-datasets.com](https://ir-datasets.com/cranfield.html) are the following metadata information:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"docs\": {\n",
    "    \"count\": 1400,\n",
    "    \"fields\": {\n",
    "      \"doc_id\": {\n",
    "        \"max_len\": 4,\n",
    "        \"common_prefix\": \"\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"queries\": {\n",
    "    \"count\": 225\n",
    "  },\n",
    "  \"qrels\": {\n",
    "    \"count\": 1837,\n",
    "    \"fields\": {\n",
    "      \"relevance\": {\n",
    "        \"counts_by_value\": {\n",
    "          \"2\": 387,\n",
    "          \"3\": 734,\n",
    "          \"4\": 363,\n",
    "          \"-1\": 225,\n",
    "          \"1\": 128\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The following subsection will outline the implementation of the information retrieval metric evaluation using the Cranfield benchmark dataset, a standard test collection in information retrieval research consisting of 1,400 ddocuments, 225 queries, and relevance judgments. First, the dataset is morphed into the expected form of the system. This is achieved by loading a placeholder inverted index and replacing the data structures stored in the Index object with those formed by Cranfield's dataset, where each document's title, text, and author fields are tokenized and indexed using the same pipeline applied to the Wikipedia corpus. Then, we perform the standard query-document retrieval pipeline for all listed queries from the benchmark, retrieving the top 1000 ranked documents for each query using cosine similarity scoring. The retrieved results are compared against Cranfield's relevance judgments, which rate documents on a scale from -1 (not relevant) to 4 (complete answer), with scores 1 and above treated as relevant for evaluation purposes. The evaluation calculates standard information retrieval metrics at multiple cutoff points to assess both the system's ability to identify relevant documents and its effectiveness at ranking them highly. The three main measures we are evaluating are precision at K (the proportion of retrieved documents that are relevant), recall at K (the proportion of relevant documents that are retrieved), and mean average precision (MAP, measuring ranking quality across all queries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab89529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c27d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Cranfield index:\n",
      "  Documents: 1400\n",
      "  Vocabulary size: 10586\n",
      "  Bigrams: 925\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from src.indexer.indexme import Index, init_bigram_idx, init_invidx_tfdf\n",
    "from src.search.searchme import tokenizer\n",
    "\n",
    "# 1. LOAD AND PREPARE CRANFIELD INDEX\n",
    "dataset = ir_datasets.load(\"cranfield\")\n",
    "\n",
    "cranfield_docidx = {\n",
    "    doc.doc_id: tokenizer(f\"{doc.title} {doc.text} {doc.author}\")\n",
    "    for doc in dataset.docs_iter()\n",
    "}\n",
    "\n",
    "cranfield_invidx = init_invidx_tfdf(cranfield_docidx)\n",
    "cranfield_bigram = init_bigram_idx(list(cranfield_invidx.keys()))\n",
    "\n",
    "dummyidx_fp = Path.cwd() / \"tests\" / \"fixtures\" / \"indexer\" / \"dummy_index.json\"\n",
    "if not dummyidx_fp.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Current working directory is misconfigured to {Path.cwd()}\\n\"\n",
    "        \"Please restart the notebook's kernel and start execution from the top.\"\n",
    "    )\n",
    "\n",
    "cranfield_idxobj = Index(dummyidx_fp)\n",
    "cranfield_idxobj.inverted_index = cranfield_invidx\n",
    "cranfield_idxobj.bigram_index = cranfield_bigram\n",
    "cranfield_idxobj.corpus_size = len(cranfield_docidx)\n",
    "\n",
    "print(f\"Loaded Cranfield index:\")\n",
    "print(f\"  Documents: {cranfield_idxobj.corpus_size}\")\n",
    "print(f\"  Vocabulary size: {len(cranfield_idxobj.inverted_index)}\")\n",
    "print(f\"  Bigrams: {len(cranfield_idxobj.bigram_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f4f433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 225 queries\n",
      "Loaded relevance judgments for 225 queries\n"
     ]
    }
   ],
   "source": [
    "# 2. LOAD QUERIES AND RELEVANCE JUDGMENTS\n",
    "\n",
    "queries = {query.query_id: query.text for query in dataset.queries_iter()}\n",
    "print(f\"\\nLoaded {len(queries)} queries\")\n",
    "\n",
    "# consider relevance >= 1 as relevant (exclude -1)\n",
    "qrels = defaultdict(set)\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.relevance >= 1:\n",
    "        qrels[qrel.query_id].add(qrel.doc_id)\n",
    "\n",
    "print(f\"Loaded relevance judgments for {len(qrels)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42141377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved results for 225 queries\n"
     ]
    }
   ],
   "source": [
    "# 3. RUN RETRIEVAL FOR ALL QUERIES\n",
    "\n",
    "def retrieve_documents(query_text, index_obj, k=1000):\n",
    "    tokens = tokenizer(query_text)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    results = index_obj.cosine_search(tokens, k=k)\n",
    "    return results  # list of (doc_id, score) tuples\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "for query_id, query_text in queries.items():\n",
    "    results = retrieve_documents(query_text, cranfield_idxobj, k=1000)\n",
    "    all_results[query_id] = results\n",
    "\n",
    "print(f\"\\nRetrieved results for {len(all_results)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. EVALUATION METRICS FUNCTIONS\n",
    "\n",
    "def get_precision(results, qrels, k=10):\n",
    "    precisions = []\n",
    "\n",
    "    for query_id, doc_list in results.items():\n",
    "        if query_id not in qrels:\n",
    "            continue\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        top_k_docs = [doc_id for doc_id, _ in doc_list[:k]]\n",
    "\n",
    "        relevant_retrieved = len(set(top_k_docs) & relevant_docs)\n",
    "        precision = relevant_retrieved / k if k > 0 else 0\n",
    "        precisions.append(precision)\n",
    "\n",
    "    return np.mean(precisions) if precisions else 0.0\n",
    "\n",
    "\n",
    "def get_recall(results, qrels, k=10):\n",
    "    recalls = []\n",
    "\n",
    "    for query_id, doc_list in results.items():\n",
    "        if query_id not in qrels:\n",
    "            continue\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        if len(relevant_docs) == 0:\n",
    "            continue\n",
    "\n",
    "        top_k_docs = [doc_id for doc_id, _ in doc_list[:k]]\n",
    "        relevant_retrieved = len(set(top_k_docs) & relevant_docs)\n",
    "\n",
    "        recall = relevant_retrieved / len(relevant_docs)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return np.mean(recalls) if recalls else 0.0\n",
    "\n",
    "\n",
    "def get_avg_precision(retrieved_docs, relevant_docs):\n",
    "    if len(relevant_docs) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    relevant_retrieved = 0\n",
    "    sum_precisions = 0.0\n",
    "\n",
    "    for i, doc_id in enumerate(retrieved_docs, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            relevant_retrieved += 1\n",
    "            precision_at_i = relevant_retrieved / i\n",
    "            sum_precisions += precision_at_i\n",
    "\n",
    "    return sum_precisions / len(relevant_docs)\n",
    "\n",
    "\n",
    "def get_map(results, qrels):\n",
    "    average_precisions = []\n",
    "\n",
    "    for query_id, doc_list in results.items():\n",
    "        if query_id not in qrels:\n",
    "            continue\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        retrieved_docs = [doc_id for doc_id, _ in doc_list]\n",
    "\n",
    "        ap = get_avg_precision(retrieved_docs, relevant_docs)\n",
    "        average_precisions.append(ap)\n",
    "\n",
    "    return np.mean(average_precisions) if average_precisions else 0.0\n",
    "\n",
    "\n",
    "def get_f1(results, qrels, k=10):\n",
    "    f1_scores = []\n",
    "\n",
    "    for query_id, doc_list in results.items():\n",
    "        if query_id not in qrels:\n",
    "            continue\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        if len(relevant_docs) == 0:\n",
    "            continue\n",
    "\n",
    "        top_k_docs = [doc_id for doc_id, _ in doc_list[:k]]\n",
    "        relevant_retrieved = len(set(top_k_docs) & relevant_docs)\n",
    "\n",
    "        precision = relevant_retrieved / k if k > 0 else 0\n",
    "        recall = relevant_retrieved / len(relevant_docs)\n",
    "\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores) if f1_scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRANFIELD EVALUATION RESULTS\n",
      "\n",
      "Mean Average Precision (MAP): 0.2656\n",
      "\n",
      "Precision@K:\n",
      "  P@  5: 0.2880\n",
      "  P@ 10: 0.2169\n",
      "  P@ 20: 0.1460\n",
      "  P@ 50: 0.0790\n",
      "  P@100: 0.0472\n",
      "\n",
      "Recall@K:\n",
      "  R@  5: 0.2408\n",
      "  R@ 10: 0.3532\n",
      "  R@ 20: 0.4704\n",
      "  R@ 50: 0.5993\n",
      "  R@100: 0.6981\n",
      "\n",
      "F1@K:\n",
      "  F1@ 5: 0.2354\n",
      "  F1@10: 0.2440\n",
      "  F1@20: 0.2063\n"
     ]
    }
   ],
   "source": [
    "# 5. CALCULATE ALL METRICS\n",
    "\n",
    "print(\"CRANFIELD EVALUATION RESULTS\")\n",
    "\n",
    "map_score = get_map(all_results, qrels)\n",
    "print(f\"\\nMean Average Precision (MAP): {map_score:.4f}\")\n",
    "\n",
    "print(\"\\nPrecision@K:\")\n",
    "for k in [5, 10, 20, 50, 100]:\n",
    "    p_at_k = get_precision(all_results, qrels, k=k)\n",
    "    print(f\"  P@{k:3d}: {p_at_k:.4f}\")\n",
    "\n",
    "print(\"\\nRecall@K:\")\n",
    "for k in [5, 10, 20, 50, 100]:\n",
    "    r_at_k = get_recall(all_results, qrels, k=k)\n",
    "    print(f\"  R@{k:3d}: {r_at_k:.4f}\")\n",
    "\n",
    "print(\"\\nF1@K:\")\n",
    "for k in [5, 10, 20]:\n",
    "    f1_at_k = get_f1(all_results, qrels, k=k)\n",
    "    print(f\"  F1@{k:2d}: {f1_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fe474",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully implemented a functional information retrieval system that demonstrates the fundamental principles of web search technology. Searchme integrates document collection, indexing, and query processing into a cohesive pipeline capable of retrieving relevant documents from a corpus of web pages. The system employs breadth-first web crawling for corpus collection, inverted indexing with positional information for efficient retrieval, TF-IDF weighting for term importance, and cosine similarity for relevance scoring. Practical optimizations including bigram-based spelling correction and magnitude caching enhance both user experience and system performance.\n",
    "\n",
    "### Evaluation Summary\n",
    "\n",
    "The Cranfield benchmark evaluation produced the following results across 225 queries and 1,400 documents:\n",
    "\n",
    "#### Ranking Quality:\n",
    "\n",
    "Mean Average Precision (MAP): 0.27\n",
    "Precision@5: 0.29, Precision@10: 0.22, Precision@20: 0.15\n",
    "\n",
    "#### Retrieval Coverage:\n",
    "\n",
    "Recall@5: 0.24, Recall@10: 0.35, Recall@20: 0.47, Recall@100: 0.70\n",
    "\n",
    "#### Combined Metrics:\n",
    "\n",
    "F1@5: 0.24, F1@10: 0.24, F1@20: 0.21\n",
    "\n",
    "These results place Searchme above random retrieval and simple Boolean systems, validating that the TF-IDF vector space model produces meaningful relevance rankings. The MAP of 0.27 exceeds typical baselines (0.15-0.20) but falls short of sophisticated ranking functions like BM25 (0.40-0.45). The relatively high recall at 100 documents (70%) demonstrates the system successfully identifies most relevant documents in the corpus, while the low precision at top ranks (22% at position 10) reveals significant weaknesses in ranking quality.\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "The system lacks several capabilities that would improve effectiveness: stemming for handling morphological variants, query expansion for addressing vocabulary mismatch, phrase matching for multi-word concepts, and field-weighted indexing for document structure awareness. The ranking model considers only term statistics without document quality signals or user interaction data. The JSON-based index persistence limits scalability to very large corpora, and the static corpus assumption prevents incremental updates.\n",
    "\n",
    "Future enhancements should prioritize implementing stemming, replacing TF-IDF with BM25, and adding query expansion. Leveraging the existing positional information for phrase queries and proximity scoring would improve precision for multi-word concepts. Performance optimizations including index compression and parallel query processing would support higher throughput and larger corpora.\n",
    "\n",
    "### Closing Remarks\n",
    "\n",
    "Searchme successfully demonstrates classical information retrieval techniques for web document search, achieving functional correctness and baseline effectiveness. The measured performance aligns with expectations for a TF-IDF system, validating both implementation and evaluation methodology. While falling short of modern search engines, the system establishes a functional foundation with well-documented limitations and clear improvement pathways. The modular architecture, comprehensive testing, and reproducible evaluation provide a solid platform for future enhancements and serve as an educational example of core IR principles in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63122fa3",
   "metadata": {},
   "source": [
    "## Source code\n",
    "\n",
    "Repository for this sytem can be found at https://github.com/yddet-www/ir-system\n",
    "\n",
    "Cranfield dataset is generously provided from https://ir-datasets.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4238938",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "1. W. B. Croft, D. Metzler, and T. Strohman, Search Engines: Information Retrieval in Practice. Boston, MA: Addison-Wesley, 2009.\n",
    "\n",
    "2. C. D. Manning, P. Raghavan, and H. Schütze, An Introduction to Information Retrieval. Cambridge, England: Cambridge University Press, 2008.\n",
    "\n",
    "3. S. MacAvaney, A. Yates, S. Feldman, D. Downey, A. Cohan, and N. Goharian, \"Simplified data wrangling with ir_datasets,\" in Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021, pp. 2429-2436.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
